{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Intro\n",
    "Big focus on choosing the right supervised learning model for that data. Acerous and non-Averous example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Intro to regression\n",
    "Basic def: take examples of labelled inputs and outputs. Given a new input, preidct it's output based on what you saw before.\n",
    "For regression: mapping <i>continuous</i> inputs to outputs.\n",
    "\n",
    "Talk of regression to the mean vs regression. Just means using a functional form to approximate datapoints. \n",
    "\n",
    "Use calculus to find least squared error for fitting a regression line. \n",
    "Theres many different error functions- absolute, squared, loss function, weighted (squashed), but squared is well behaved as it's smooth for a costant and thus you can find minimum error more easily. This is due to the calculus behind it- taking derivatives means chain rule -> becomes a smooth linear problem! Then you can set to 0 to get the minimum (optimum) and this is just GCSE calculus. Then you can find smallest error for line and that's your least squares error. Becomes 1 equation in 1 unknown. See 'least squares calculus' screenshot. Basically the constant is the mean - so you add all the Ys together and find the mean distance from the line and solve. This is why least squares is simple, also generalises well to higher dimensions.  \n",
    "\n",
    "\n",
    "Fitting polynomial can't go to order beyond number of data points that you have. The higher the polynomial order the better it'll likely fit your data, however obviously OVERFITTING- 'over-commits to the data'. See screenshot for order 8. Best constant in terms of quared error is the mean. \n",
    "\n",
    "## Linear & polynomial regression\n",
    "You're trying to find the coefficients for n order polynomial:\n",
    "C0 + C1 X + C2 X^2 + C3 X^3 +... ~ Y\n",
    "\n",
    "Way to solve this: linear algebra. Make matrix of all x points * all C values, equals Y. Solve for C matrix. Do simple matrix manipulation: times both sides by Xtranspose. You have to modelling not just F but F + E (E = error terms). This recognises the training data is going to contain some error. \n",
    "\n",
    "Transcirption error = human error. VBIt more specific than just noise e.g. switching numbers around etc. \n",
    "Unmodeled influences = error where you're not including other factors affecting prediction. \n",
    "\n",
    "IID data = the data you're training and testing upon should be independent and identically distributed. Fundamental assumption in quite a few algorithms.\n",
    "\n",
    "## Model selection & overfitting with cross-validation\n",
    "\n",
    "Aim: use a model that is complex enough to fit the data but without causing problems on the test set. Cross validation - SPLIT YOUR TRAINING SET (!!) and apportion some of it to be your temporary test set. Hold out some of the training set as a trial test set - this is the cross validation set. Stand in for actual test data. \n",
    "\n",
    "Using cross validation for your error vs polynomial degree will throw up overfitting errors that you wouldn't otherwise see from a single train-test split. General inverted U shape of error vs increasing polynomial degree when you use CV, if you dont use CV you just see constant decreasing and could be fooled into overfitting - see screenshot 'cross validation polynomial fitting'. The left of the U represents underfitting- see underfit vs overfit polynomial degree cross validation screenshot. \n",
    "\n",
    "So you could look to minimise validation set error. \n",
    "\n",
    "## Representation and encoding in regression\n",
    "You could also have vector continuous input into regression. Regressing to a plane rather than a curve. You can feed in discrete data too if you encode... bt have to be careful with implying an 'order' to categorical vars so one hot encoding is sometime sbetter. Scalar, vector, discrete. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised regression learning - Lesson 2\n",
    "\n",
    "## Parametric vs instance based methods\n",
    "Instance based approach- k nearest neighbour for regression (see screenshot). ID the K nearest neighbours, take the mean of their Y values for the inputted X value. You can then draw a curve through all of these and itll fit the data pretty well. This is a slightly different approach to the curve fitting we're used to- known as <i>parametic models</i>. \n",
    "\n",
    "Kernel regression is another instance based/data centric approach, but kernel weights the contributions of each datapoint according to how different they are. \n",
    "\n",
    "### When to use parametric or non-parametric\n",
    "Different sorts of problems - physics based model in which we know a mathematical relationship exists e.g. angle of canon vs canonball distance (well defined trajectory), vs. for example the number of bees vs how much food you put out (hard to model mathematically). \n",
    "\n",
    "The canon ball is BIASED (you can start with an estimate for the underlying behaviour of system through maths equation), the honey bees problem is UNBIASED (you can't really guess at the underlying behaviour- could be any shape). For aprametric: training is slow but querying is fast. Dont have to store the original data so it's efficient for qwuerying but harder to update the model as new data comes in- complete re-run needed. \n",
    "\n",
    "For non parametric/instance based, store all data points so slow for large datasets however new evidence added easily. No paramerters need to be learned so no additional time needed. Training fast, querying slow. These also avoid having to assume an underlying model so good for complex patterns when we don't know what it's supposed to look like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous supervised learning & sklearn- Lesson 3/4\n",
    "Continuous refers to the <b>output</b> being continuous.\n",
    "\n",
    "Generalised linear models. Sklearn uses orindary least squares for LinearRegresson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5  0.5]\n",
      "2.22044604925e-16\n",
      "0.464285714286\n"
     ]
    }
   ],
   "source": [
    "# Code structure\n",
    "\n",
    "xtrain = [[0,0], [1, 1], [2, 2]]\n",
    "ytrain = [0, 1, 2]\n",
    "\n",
    "xtest = [[0,1],[1,1], [2,1]]\n",
    "ytest = [0,1,3]\n",
    "\n",
    "# 1. Import statement\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 2. Define classifier\n",
    "classifier = linear_model.LinearRegression()\n",
    "\n",
    "# 3. Fit classifier\n",
    "classifier.fit(xtrain, ytrain) # Simply takes 2 arguments: the training features and the training targets\n",
    "\n",
    "# Display coefficients\n",
    "print(classifier.coef_)\n",
    "print(classifier.intercept_)\n",
    "print(classifier.score(xtest, ytest)) ## Usually the higher the R squared score the better. \n",
    "#Obviously there's some caveats that are widely discussed online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Linear regression errors: WHY SSE? WHY SUM OF SQUARES?\n",
    " Good to visualise by plotting scatter and regression line.\n",
    " \n",
    " Error can be negative. Hence why mean/sum of ABSOLUTE error good, and squared. Makes it positive. \n",
    " Linear regression minimises the sum of squared errors. Actual - predicted squared, summed. \n",
    " \n",
    " Algorithms for doing this include ordinatry least squares (sklearn LR uses this)\n",
    " Also gradient descent.\n",
    " \n",
    " Minimizing SSE is beter than just absolute error because:\n",
    " - Imagine 3 lines both splitting the regression data, but ones higher, ones lower and ones in the middle- see screenshot 'minimizing sse'\n",
    " - Middle provides us a better regression line because its equally spaced\n",
    " - Sum of absolute error comes out the same for all 3 so misses this out! Whereas squared would capture it.\n",
    " - Theres a fundamental ambiguity when you use MAE because it doesnt capture the variation in error.\n",
    " - However, squaring amplifies this so will penalise more. \n",
    " \n",
    " \n",
    "However, SSE not perfect... if you have less data, it willl perform better as less data points = less overall sum of errors. Even for the same line! See screenshot SSE isnt perfect.\n",
    "\n",
    "THEREFORE R squared - doesnt have this shortfall. Described the <b>goodness of fit</b> for a linear regression. Is independt of number of points in dataset. \n",
    "\n",
    "0.85 = good R squared. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah.barrington\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ec55a71a7381>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Sarah's prediction: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sarah.barrington\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 512\u001b[1;33m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sarah.barrington\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sarah.barrington\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 181\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 10]"
     ]
    }
   ],
   "source": [
    "## Key learning: ALWAYS Visualise results as you go along\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy \n",
    "\n",
    "# import your data as x and y \n",
    "\n",
    "# GENERAL CODE STRUCTURE FOR VISUALISATION OF REGRESSION\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(x, y)\n",
    "print \"Sarah's prediction: \", reg.predict([input])\n",
    "\n",
    "print reg.score(x, y)\n",
    "print reg.coef_\n",
    "print reg.intercept_\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(ages, reg.predict(ages), color = 'blue', linewidth = 3)\n",
    "plt.xlabel(\"X label\")\n",
    "plt.ylabel(\"Y label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes a good linear regression?\n",
    "Not just the fact it's a straight line- for example, a single vertical line isnt a good fit. Why? Because inputting the X value doesnt indicate which Y value it could be- it could be infinte Y values. A good linear reression is one that you can vary X and get a result in Y. Otherwise it's Y = infinte X!\n",
    "Horizontal line, however, is OK because no variation in Y (despite variation in X). So you always know your output. Always need a RANGE of x values. \n",
    "\n",
    "### Comparing classification and regression\n",
    "- See screenshot comparing classification and regression\n",
    "\n",
    "### Multivariate regression (the usual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
